version: '3'

# NOTE: Task doesn't allow to override environment variables. Thus, when an
# environment variable is referenced in a taskfile, it is because we expect it
# to be defined by the environment where Task is being invoked, in an `.env`
# file, or in a task's `env:` attribute.
dotenv: [".env", '{{.ENV}}/.env.']

vars:
  BINDIR: .local/bin

  # Configuration Defaults
  NAMESPACE: rn
  RELEASE: rp
  KIND_CLUSTERNAME: rp-helm
  CLUSTER_NAME: '{{.CLUSTER_NAME | default "capi"}}'
  CLUSTER_NAMESPACE: '{{.CLUSTER_NAMESPACE | default "default"}}'

  # Overridable task vars
  HELM_OPTIONS: ""
  KIND_FLAGS: "--config .github/kind.yaml"

  SRC_DIR:
    sh: realpath {{default "." .SRC_DIR}}

includes:
  tool:
    taskfile: tasks/ToolTasks.yaml
    vars:
      BINDIR: "{{.BINDIR}}"

# if a task is referenced multiple times, only run it once
run: once

tasks:

  up:
    cmds:
      - task: kind-create
      - task: install-redpanda-chart

  add-jetstack-repo:
    cmds:
      - helm repo add jetstack https://charts.jetstack.io
      - helm repo update
    status:
      - helm search repo -r '\vjetstack/cert-manager\v' | grep cert-manager

  install-cert-manager:
    deps:
      - add-jetstack-repo
    cmds:
      - helm install cert-manager jetstack/cert-manager --set installCRDs=true --namespace cert-manager --create-namespace

  aws-check-login:
    internal: true
    cmds:
      - aws sts get-caller-identity > /dev/null

  kind-create:
    cmds:
      - kind create cluster --name {{.KIND_CLUSTERNAME}} {{.KIND_FLAGS}}
      - task: install-cert-manager
    status:
      - "kind get clusters | grep {{.KIND_CLUSTERNAME}}"

  kind-delete:
    cmds:
      - kind delete cluster --name {{.KIND_CLUSTERNAME}}

  install-redpanda-chart:
    aliases:
      - upgrade-redpanda-chart
    cmds:
      - helm upgrade --install {{ .RELEASE }} ./charts/redpanda --namespace {{ .NAMESPACE }} --create-namespace --wait --debug {{ .HELM_OPTIONS }}

  uninstall-redpanda-chart:
    cmds:
      - helm uninstall {{ .RELEASE }} -n {{ .NAMESPACE }} --wait || true
      - kubectl -n {{ .NAMESPACE }} delete pods --all --grace-period=0 --force --wait=false || true
      - kubectl -n {{ .NAMESPACE }} delete pvc --all --force --grace-period=0 --wait=false || true
      - kubectl delete ns {{ .NAMESPACE }}

  minikube-start:
    cmds:
      - minikube start --nodes=4
      - kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml
      - ./scripts/change-default-sc.sh
      - task: install-cert-manager

  minikube-delete:
    cmds:
      - minikube delete

  capi-bootstrap-aws:
    deps:
      - tool:clusterctl
    env:
      EXP_MACHINE_POOL: true
      CAPA_EKS_ADD_ROLES: "{{.CAPA_EKS_ADD_ROLES}}"
      CAPA_EKS_IAM: "{{.CAPA_EKS_IAM}}"
      AWS_B64ENCODED_CREDENTIALS: "{{.AWS_B64ENCODED_CREDENTIALS}}"
    vars:
      CAPI_INFRASTRUCTURE: '{{ default "unknown" .CAPI_INFRASTRUCTURE }}'
    cmds:
      - task: kind-create
        vars:
          KIND_CLUSTERNAME: bootstrap
          KIND_FLAGS: ""
      - "{{.BINDIR}}/clusterctl init -i {{ .CAPI_INFRASTRUCTURE }} -v7 --wait-providers"

  capi-bootstrap-gke:
    deps:
      - tool:clusterctl
      - tool:auth-gcp
    env:
      GCP_B64ENCODED_CREDENTIALS: "{{.GCP_B64ENCODED_CREDENTIALS}}"
      EXP_CAPG_GKE: true
      EXP_MACHINE_POOL: true
    vars:
      CAPI_INFRASTRUCTURE: gcp
    cmds:
      - task: kind-create
        vars:
          KIND_CLUSTERNAME: bootstrap
          KIND_FLAGS: ""
      - "{{.BINDIR}}/clusterctl init -i {{ .CAPI_INFRASTRUCTURE }} -v7 --wait-providers"

  capi-create-eks:
    deps:
      - aws-check-login
      - tool:clusterawsadm
    env:
      CAPA_EKS_IAM: true
      CAPA_EKS_ADD_ROLES: true
    cmds:
      - task: capi-bootstrap-aws
        vars:
          CAPI_INFRASTRUCTURE: aws
          CAPA_EKS_IAM: true
          CAPA_EKS_ADD_ROLES: true
          AWS_B64ENCODED_CREDENTIALS:
            sh: "{{.BINDIR}}/clusterawsadm bootstrap credentials encode-as-profile 2>/dev/null"
      - "{{.BINDIR}}/clusterawsadm bootstrap iam create-cloudformation-stack --config=.buildkite/capi/eks-bootstrap.yaml"
      - helm install -n {{.CLUSTER_NAMESPACE}} {{.CLUSTER_NAME}} .buildkite/capi/eks-cluster --create-namespace  --debug
      - kubectl wait --for=condition=ready cluster {{.CLUSTER_NAME}}-eks-cluster --timeout=40m
      - kubectl wait --for=condition=Ready machinepool {{.CLUSTER_NAME}}-eks-cluster-pool-0 --timeout=20m

  capi-create-gke:
    env:
      # this is done by running the following:
      # export GCP_B64ENCODED_CREDENTIALS=$(base64 < "${GOOGLE_APPLICATION_CREDENTIALS}" | tr -d '\n')
      GCP_B64ENCODED_CREDENTIALS: "{{.GCP_B64ENCODED_CREDENTIALS}}"
      PROJECT_ID: "{{.PROJECT_ID}}"
    cmds:
      - task: capi-bootstrap-gke
        vars:
          CAPI_INFRASTRUCTURE: gcp
      - helm install -n {{.CLUSTER_NAMESPACE}} {{.CLUSTER_NAME}} .buildkite/capi/gke-cluster --create-namespace --set projectID={{.PROJECT_ID}} --debug
      - kubectl wait --for=condition=ready cluster {{.CLUSTER_NAME}}-gke-cluster --timeout=40m
      - kubectl wait --for=condition=Ready machinepool {{.CLUSTER_NAME}}-gke-cluster-mp-0 --timeout=20m

  capi-delete:
    internal: true
    cmds:
      - kubectl delete -f {{.CLUSTERFILE}} --wait

  capi-delete-eks:
    env:
      AWS_B64ENCODED_CREDENTIALS:
        sh: "{{.BINDIR}}/clusterawsadm bootstrap credentials encode-as-profile 2> /dev/null"
    cmds:
      - "{{.BINDIR}}/clusterawsadm controller update-credentials"
      - helm uninstall --wait -n {{.CLUSTER_NAMESPACE}} {{.CLUSTER_NAME}} --timeout 60m

  capi-delete-gke:
    deps:
      - tool:kubectl
    env:
      GCP_B64ENCODED_CREDENTIALS: "{{.GCP_B64ENCODED_CREDENTIALS}}"
    cmds:
      - kubectl delete cluster {{.CLUSTER_NAME}}-gke-cluster --timeout=20m
      - helm uninstall --wait -n {{.CLUSTER_NAMESPACE}} {{.CLUSTER_NAME}} --timeout 10m --debug

  # This is if you are running locally and not in CI
  gke-auth-login:
    deps:
      - tool:gcloud
    env:
      PROJECT_ID: "{{.PROJECT_ID}}"
      GOOGLE_APPLICATION_CREDENTIALS: "{{.GOOGLE_APPLICATION_CREDENTIALS}}"
    cmds:
      - gcloud auth activate-service-account --key-file={{.GOOGLE_APPLICATION_CREDENTIALS}}
      - gcloud config set project {{.PROJECT_ID}} --quiet

  gke-get-kubeconfig:
    deps:
      - tool:gcloud
      - tool:auth-gcp
      - tool:gcloud-auth-plugin
    env:
      GOOGLE_APPLICATION_CREDENTIALS: "{{.GOOGLE_APPLICATION_CREDENTIALS}}"
      PROJECT_ID: "{{.PROJECT_ID}}"
      USE_GKE_GCLOUD_AUTH_PLUGIN: True
      KUBECONFIG: '{{ default "capi-gke-cluster.conf" .KUBECONFIG}}'
    cmds:
      - gcloud container clusters get-credentials {{.CLUSTER_NAME}}-gke-cluster --region=us-west1 --project {{.PROJECT_ID}}
